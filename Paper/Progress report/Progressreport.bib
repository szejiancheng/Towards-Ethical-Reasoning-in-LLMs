
@misc{vishwanath_reinforcement_2024,
	title = {Reinforcement {Learning} and {Machine} ethics:a systematic review},
	shorttitle = {Reinforcement {Learning} and {Machine} ethics},
	url = {http://arxiv.org/abs/2407.02425},
	doi = {10.48550/arXiv.2407.02425},
	abstract = {Machine ethics is the field that studies how ethical behaviour can be accomplished by autonomous systems. While there exist some systematic reviews aiming to consolidate the state of the art in machine ethics prior to 2020, these tend to not include work that uses reinforcement learning agents as entities whose ethical behaviour is to be achieved. The reason for this is that only in the last years we have witnessed an increase in machine ethics studies within reinforcement learning. We present here a systematic review of reinforcement learning for machine ethics and machine ethics within reinforcement learning. Additionally, we highlight trends in terms of ethics specifications, components and frameworks of reinforcement learning, and environments used to result in ethical behaviour. Our systematic review aims to consolidate the work in machine ethics and reinforcement learning thus completing the gap in the state of the art machine ethics landscape},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Vishwanath, Ajay and Dennis, Louise A. and Slavkovik, Marija},
	month = jul,
	year = {2024},
	note = {arXiv:2407.02425 
version: 1},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\7ELKLCBE\\Vishwanath et al. - 2024 - Reinforcement Learning and Machine ethicsa system.pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\6TNB46JG\\2407.html:text/html},
}

@misc{tolmeijer_implementations_2021,
	title = {Implementations in {Machine} {Ethics}: {A} {Survey}},
	shorttitle = {Implementations in {Machine} {Ethics}},
	url = {http://arxiv.org/abs/2001.07573},
	doi = {10.48550/arXiv.2001.07573},
	abstract = {Increasingly complex and autonomous systems require machine ethics to maximize the benefits and minimize the risks to society arising from the new technology. It is challenging to decide which type of ethical theory to employ and how to implement it effectively. This survey provides a threefold contribution. First, it introduces a trimorphic taxonomy to analyze machine ethics implementations with respect to their object (ethical theories), as well as their nontechnical and technical aspects. Second, an exhaustive selection and description of relevant works is presented. Third, applying the new taxonomy to the selected works, dominant research patterns, and lessons for the field are identified, and future directions for research are suggested.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Tolmeijer, Suzanne and Kneer, Markus and Sarasua, Cristina and Christen, Markus and Bernstein, Abraham},
	month = jan,
	year = {2021},
	note = {arXiv:2001.07573},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\GAEUK6EB\\Tolmeijer et al. - 2021 - Implementations in Machine Ethics A Survey.pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\K4TT8X4I\\2001.html:text/html},
}

@article{wallach_machine_2008,
	title = {Machine {Morality}: {Bottom}-{Up} and {Top}-{Down} {Approaches} for {Modelling} {Human} {Moral} {Faculties}},
	volume = {22},
	shorttitle = {Machine {Morality}},
	doi = {10.1007/s00146-007-0099-0},
	number = {4},
	journal = {AI and Society},
	author = {Wallach, Wendell and Allen, Colin and Smit, Iva},
	year = {2008},
	note = {Publisher: Springer London},
	pages = {565--582},
	file = {Snapshot:C\:\\Users\\JC\\Zotero\\storage\\H2S5MLWQ\\WENMMB.html:text/html},
}

@book{kant_groundwork_1785,
	address = {New York},
	title = {Groundwork for the {Metaphysics} of {Morals}},
	publisher = {Oxford University Press},
	author = {Kant, Immanuel},
	editor = {Hill, Thomas E. and Zweig, Arnulf},
	year = {1785},
	file = {Snapshot:C\:\\Users\\JC\\Zotero\\storage\\G76W6NYP\\KANGFT.html:text/html},
}

@incollection{alexander_deontological_2021,
	edition = {Winter 2021},
	title = {Deontological {Ethics}},
	url = {https://plato.stanford.edu/archives/win2021/entries/ethics-deontological/},
	abstract = {The word deontology derives from the Greek words for duty(deon) and science (or study) of (logos). Incontemporary moral philosophy, deontology is one of those kinds ofnormative theories regarding which choices are morally required,forbidden, or permitted. In other words, deontology falls within thedomain of moral theories that guide and assess our choices of what weought to do (deontic theories), in contrast to those that guide andassess what kind of person we are and should be (aretaic [virtue]theories). And within the domain of moral theories that assess ourchoices, deontologists—those who subscribe to deontologicaltheories of morality—stand in opposition toconsequentialists.},
	urldate = {2024-11-05},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Alexander, Larry and Moore, Michael},
	editor = {Zalta, Edward N.},
	year = {2021},
	keywords = {consequentialism: rule, doing vs. allowing harm, double effect, doctrine of, ethics: virtue, Kant, Immanuel: moral philosophy, Moore, George Edward: moral philosophy, moral dilemmas},
	file = {SEP - Snapshot:C\:\\Users\\JC\\Zotero\\storage\\MHU43NF3\\ethics-deontological.html:text/html},
}

@incollection{sinnott-armstrong_consequentialism_2023,
	edition = {Winter 2023},
	title = {Consequentialism},
	url = {https://plato.stanford.edu/archives/win2023/entries/consequentialism/},
	abstract = {Consequentialism, as its name suggests, is simply the view thatnormative properties depend only on consequences. This historicallyimportant and still popular theory embodies the basic intuition thatwhat is best or right is whatever makes the world best in the future,because we cannot change the past, so worrying about the past is nomore useful than crying over spilled milk. This general approach canbe applied at different levels to different normative properties ofdifferent kinds of things, but the most prominent example is probablyconsequentialism about the moral rightness of acts, which holds thatwhether an act is morally right depends only on the consequences ofthat act or of something related to that act, such as the motivebehind the act or a general rule requiring acts of the same kind.},
	urldate = {2024-11-05},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Sinnott-Armstrong, Walter},
	editor = {Zalta, Edward N. and Nodelman, Uri},
	year = {2023},
	keywords = {Bentham, Jeremy, consequentialism: rule, consequentializing, hedonism, Mill, John Stuart, Moore, George Edward, reasons for action: agent-neutral vs. agent-relative, Sidgwick, Henry},
	file = {SEP - Snapshot:C\:\\Users\\JC\\Zotero\\storage\\JWQ8222P\\consequentialism.html:text/html},
}

@incollection{hursthouse_virtue_2023,
	edition = {Fall 2023},
	title = {Virtue {Ethics}},
	url = {https://plato.stanford.edu/archives/fall2023/entries/ethics-virtue/},
	abstract = {Virtue ethics is currently one of three major approaches in normativeethics. It may, initially, be identified as the one that emphasizesthe virtues, or moral character, in contrast to the approach thatemphasizes duties or rules (deontology) or that emphasizes theconsequences of actions (consequentialism). Suppose it is obvious thatsomeone in need should be helped. A utilitarian will point to the factthat the consequences of doing so will maximize well-being, adeontologist to the fact that, in doing so the agent will be acting inaccordance with a moral rule such as “Do unto others as youwould be done by” and a virtue ethicist to the fact that helpingthe person would be charitable or benevolent.},
	urldate = {2024-11-05},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Hursthouse, Rosalind and Pettigrove, Glen},
	editor = {Zalta, Edward N. and Nodelman, Uri},
	year = {2023},
	keywords = {Aristotle, character, moral, character, moral: empirical approaches, consequentialism, ethics: deontological, moral dilemmas},
	file = {SEP - Snapshot:C\:\\Users\\JC\\Zotero\\storage\\7VW3KXDG\\ethics-virtue.html:text/html},
}

@misc{jiang_can_2022,
	title = {Can {Machines} {Learn} {Morality}? {The} {Delphi} {Experiment}},
	shorttitle = {Can {Machines} {Learn} {Morality}?},
	url = {http://arxiv.org/abs/2110.07574},
	doi = {10.48550/arXiv.2110.07574},
	abstract = {As AI systems become increasingly powerful and pervasive, there are growing concerns about machines' morality or a lack thereof. Yet, teaching morality to machines is a formidable task, as morality remains among the most intensely debated questions in humanity, let alone for AI. Existing AI systems deployed to millions of users, however, are already making decisions loaded with moral implications, which poses a seemingly impossible challenge: teaching machines moral sense, while humanity continues to grapple with it. To explore this challenge, we introduce Delphi, an experimental framework based on deep neural networks trained directly to reason about descriptive ethical judgments, e.g., "helping a friend" is generally good, while "helping a friend spread fake news" is not. Empirical results shed novel insights on the promises and limits of machine ethics; Delphi demonstrates strong generalization capabilities in the face of novel ethical situations, while off-the-shelf neural network models exhibit markedly poor judgment including unjust biases, confirming the need for explicitly teaching machines moral sense. Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and inconsistencies. Despite that, we demonstrate positive use cases of imperfect Delphi, including using it as a component model within other imperfect AI systems. Importantly, we interpret the operationalization of Delphi in light of prominent ethical theories, which leads us to important future research questions.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Jiang, Liwei and Hwang, Jena D. and Bhagavatula, Chandra and Bras, Ronan Le and Liang, Jenny and Dodge, Jesse and Sakaguchi, Keisuke and Forbes, Maxwell and Borchardt, Jon and Gabriel, Saadia and Tsvetkov, Yulia and Etzioni, Oren and Sap, Maarten and Rini, Regina and Choi, Yejin},
	month = jul,
	year = {2022},
	note = {arXiv:2110.07574},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\FVQFGQVC\\Jiang et al. - 2022 - Can Machines Learn Morality The Delphi Experiment.pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\U73QCW3Z\\2110.html:text/html},
}
@article{asimov1942runaround,
  author  = {Isaac Asimov},
  title   = {Runaround},
  journal = {Astounding Science Fiction},
  volume  = {29},
  number  = {1},
  pages   = {94--103},
  year    = {1942},
  month   = {March}
}

@misc{rao_ethical_2023,
	title = {Ethical {Reasoning} over {Moral} {Alignment}: {A} {Case} and {Framework} for {In}-{Context} {Ethical} {Policies} in {LLMs}},
	shorttitle = {Ethical {Reasoning} over {Moral} {Alignment}},
	url = {http://arxiv.org/abs/2310.07251},
	abstract = {In this position paper, we argue that instead of morally aligning LLMs to specific set of ethical principles, we should infuse generic ethical reasoning capabilities into them so that they can handle value pluralism at a global scale. When provided with an ethical policy, an LLM should be capable of making decisions that are ethically consistent to the policy. We develop a framework that integrates moral dilemmas with moral principles pertaining to different foramlisms of normative ethics, and at different levels of abstractions. Initial experiments with GPT-x models shows that while GPT-4 is a nearly perfect ethical reasoner, the models still have bias towards the moral values of Western and English speaking societies.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Rao, Abhinav and Khandelwal, Aditi and Tanmay, Kumar and Agarwal, Utkarsh and Choudhury, Monojit},
	month = oct,
	year = {2023},
	note = {arXiv:2310.07251},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\28ULWARR\\Rao et al. - 2023 - Ethical Reasoning over Moral Alignment A Case and.pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\ZCQXXD2Z\\2310.html:text/html},
}


@misc{zhao_ethical-advice_2021,
	title = {Ethical-{Advice} {Taker}: {Do} {Language} {Models} {Understand} {Natural} {Language} {Interventions}?},
	shorttitle = {Ethical-{Advice} {Taker}},
	url = {http://arxiv.org/abs/2106.01465},
	doi = {10.48550/arXiv.2106.01465},
	abstract = {Is it possible to use natural language to intervene in a model's behavior and alter its prediction in a desired way? We investigate the effectiveness of natural language interventions for reading-comprehension systems, studying this in the context of social stereotypes. Specifically, we propose a new language understanding task, Linguistic Ethical Interventions (LEI), where the goal is to amend a question-answering (QA) model's unethical behavior by communicating context-specific principles of ethics and equity to it. To this end, we build upon recent methods for quantifying a system's social stereotypes, augmenting them with different kinds of ethical interventions and the desired model behavior under such interventions. Our zero-shot evaluation finds that even today's powerful neural language models are extremely poor ethical-advice takers, that is, they respond surprisingly little to ethical interventions even though these interventions are stated as simple sentences. Few-shot learning improves model behavior but remains far from the desired outcome, especially when evaluated for various types of generalization. Our new task thus poses a novel language understanding challenge for the community.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Zhao, Jieyu and Khashabi, Daniel and Khot, Tushar and Sabharwal, Ashish and Chang, Kai-Wei},
	month = jun,
	year = {2021},
	note = {arXiv:2106.01465},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\TBTS9425\\Zhao et al. - 2021 - Ethical-Advice Taker Do Language Models Understan.pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\QTYC85IX\\2106.html:text/html},
}

@inproceedings{ziems_moral_2022,
	address = {Dublin, Ireland},
	title = {The {Moral} {Integrity} {Corpus}: {A} {Benchmark} for {Ethical} {Dialogue} {Systems}},
	shorttitle = {The {Moral} {Integrity} {Corpus}},
	url = {https://aclanthology.org/2022.acl-long.261},
	doi = {10.18653/v1/2022.acl-long.261},
	abstract = {Conversational agents have come increasingly closer to human competence in open-domain dialogue settings; however, such models can reflect insensitive, hurtful, or entirely incoherent viewpoints that erode a user's trust in the moral integrity of the system. Moral deviations are difficult to mitigate because moral judgments are not universal, and there may be multiple competing judgments that apply to a situation simultaneously. In this work, we introduce a new resource, not to authoritatively resolve moral ambiguities, but instead to facilitate systematic understanding of the intuitions, values and moral judgments reflected in the utterances of dialogue systems. The Moral Integrity Corpus, MIC, is such a resource, which captures the moral assumptions of 38k prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs). Each RoT reflects a particular moral conviction that can explain why a chatbot's reply may appear acceptable or problematic. We further organize RoTs with a set of 9 moral and social attributes and benchmark performance for attribute classification. Most importantly, we show that current neural language models can automatically generate new RoTs that reasonably describe previously unseen interactions, but they still struggle with certain scenarios. Our findings suggest that MIC will be a useful resource for understanding and language models' implicit moral assumptions and flexibly benchmarking the integrity of conversational agents. To download the data, see https://github.com/GT-SALT/mic},
	urldate = {2024-11-05},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ziems, Caleb and Yu, Jane and Wang, Yi-Chia and Halevy, Alon and Yang, Diyi},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {3755--3773},
	file = {Full Text PDF:C\:\\Users\\JC\\Zotero\\storage\\DEYBRL5X\\Ziems et al. - 2022 - The Moral Integrity Corpus A Benchmark for Ethica.pdf:application/pdf},
}

@misc{schick_self-diagnosis_2021,
	title = {Self-{Diagnosis} and {Self}-{Debiasing}: {A} {Proposal} for {Reducing} {Corpus}-{Based} {Bias} in {NLP}},
	shorttitle = {Self-{Diagnosis} and {Self}-{Debiasing}},
	url = {http://arxiv.org/abs/2103.00453},
	doi = {10.48550/arXiv.2103.00453},
	abstract = {When trained on large, unfiltered crawls from the internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: they often generate racist, sexist, violent or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model's parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Schick, Timo and Udupa, Sahana and Schütze, Hinrich},
	month = sep,
	year = {2021},
	note = {arXiv:2103.00453},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\C3STB6RX\\Schick et al. - 2021 - Self-Diagnosis and Self-Debiasing A Proposal for .pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\5TUYMNA3\\2103.html:text/html},
}

@inproceedings{liu_aligning_2022,
	address = {Seattle, United States},
	title = {Aligning {Generative} {Language} {Models} with {Human} {Values}},
	url = {https://aclanthology.org/2022.findings-naacl.18},
	doi = {10.18653/v1/2022.findings-naacl.18},
	abstract = {Although current large-scale generative language models (LMs) can show impressive insights about factual knowledge, they do not exhibit similar success with respect to human values judgements (e.g., whether or not the generations of an LM are moral). Existing methods learn human values either by directly mimicking the behavior of human data, or rigidly constraining the generation space to human-chosen tokens. These methods are inherently limited in that they do not consider the contextual and abstract nature of human values and as a result often fail when dealing with out-of-domain context or sophisticated and abstract human values. This paper proposes SENSEI, a new reinforcement learning based method that can embed human values judgements into each step of language generation. SENSEI deploys an Actor-Critic framework, where the Critic is a reward distributor that simulates the reward assignment procedure of humans, while the Actor guides the generation towards the maximum reward direction. Compared with five existing methods in three human values alignment datasets, SENSEI not only achieves higher alignment performance in terms of both automatic and human evaluations, but also shows improvements on robustness and transfer learning on unseen human values.},
	urldate = {2024-11-05},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Ruibo and Zhang, Ge and Feng, Xinyu and Vosoughi, Soroush},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	pages = {241--252},
	file = {Full Text PDF:C\:\\Users\\JC\\Zotero\\storage\\87QFY6AV\\Liu et al. - 2022 - Aligning Generative Language Models with Human Val.pdf:application/pdf},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\FC7ZZIEH\\Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\8KWW4DPH\\2201.html:text/html},
}

@misc{yao_tree_2023,
	title = {Tree of {Thoughts}: {Deliberate} {Problem} {Solving} with {Large} {Language} {Models}},
	shorttitle = {Tree of {Thoughts}},
	url = {http://arxiv.org/abs/2305.10601},
	doi = {10.48550/arXiv.2305.10601},
	abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
	month = dec,
	year = {2023},
	note = {arXiv:2305.10601},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\ZB2A7CLE\\Yao et al. - 2023 - Tree of Thoughts Deliberate Problem Solving with .pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\2RF9W4GG\\2305.html:text/html},
}

@misc{besta_graph_2024,
	title = {Graph of {Thoughts}: {Solving} {Elaborate} {Problems} with {Large} {Language} {Models}},
	shorttitle = {Graph of {Thoughts}},
	url = {http://arxiv.org/abs/2308.09687},
	doi = {10.48550/arXiv.2308.09687},
	abstract = {We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62\% over ToT, while simultaneously reducing costs by {\textgreater}31\%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
	month = feb,
	year = {2024},
	note = {arXiv:2308.09687},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\IH8HK6LX\\Besta et al. - 2024 - Graph of Thoughts Solving Elaborate Problems with.pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\4BAAZV73\\2308.html:text/html},
}

@misc{noauthor_230312712_nodate,
	title = {[2303.12712] {Sparks} of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
	url = {https://arxiv.org/abs/2303.12712},
	urldate = {2024-11-05},
	file = {[2303.12712] Sparks of Artificial General Intelligence\: Early experiments with GPT-4:C\:\\Users\\JC\\Zotero\\storage\\ATYEIV73\\2303.html:text/html},
}

@misc{ganguli_capacity_2023,
	title = {The {Capacity} for {Moral} {Self}-{Correction} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2302.07459},
	doi = {10.48550/arXiv.2302.07459},
	abstract = {We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to "morally self-correct" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Ganguli, Deep and Askell, Amanda and Schiefer, Nicholas and Liao, Thomas I. and Lukošiūtė, Kamilė and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and Olsson, Catherine and Hernandez, Danny and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kernion, Jackson and Kerr, Jamie and Mueller, Jared and Landau, Joshua and Ndousse, Kamal and Nguyen, Karina and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Mercado, Noemi and DasSarma, Nova and Rausch, Oliver and Lasenby, Robert and Larson, Robin and Ringer, Sam and Kundu, Sandipan and Kadavath, Saurav and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and Telleen-Lawton, Timothy and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Olah, Christopher and Clark, Jack and Bowman, Samuel R. and Kaplan, Jared},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07459},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\UB4SH8ZP\\Ganguli et al. - 2023 - The Capacity for Moral Self-Correction in Large La.pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\9QHW8NXV\\2302.html:text/html},
}

@misc{pan_logic-lm_2023,
	title = {Logic-{LM}: {Empowering} {Large} {Language} {Models} with {Symbolic} {Solvers} for {Faithful} {Logical} {Reasoning}},
	shorttitle = {Logic-{LM}},
	url = {http://arxiv.org/abs/2305.12295},
	doi = {10.48550/arXiv.2305.12295},
	abstract = {Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver's error messages to revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2\% over using LLM alone with standard prompting and 18.4\% over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning. Code and data are publicly available at https://github.com/teacherpeterpan/Logic-LLM.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Pan, Liangming and Albalak, Alon and Wang, Xinyi and Wang, William Yang},
	month = oct,
	year = {2023},
	note = {arXiv:2305.12295},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\XME3ANZI\\Pan et al. - 2023 - Logic-LM Empowering Large Language Models with Sy.pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\U4LZ2C2Y\\2305.html:text/html},
}

@misc{lewis_retrieval-augmented_2021,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	url = {http://arxiv.org/abs/2005.11401},
	doi = {10.48550/arXiv.2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	month = apr,
	year = {2021},
	note = {arXiv:2005.11401},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\I4WLADX3\\Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Inten.pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\XH5FU8C8\\2005.html:text/html},
}

@misc{gao_pal_2023,
	title = {{PAL}: {Program}-aided {Language} {Models}},
	shorttitle = {{PAL}},
	url = {http://arxiv.org/abs/2211.10435},
	doi = {10.48550/arXiv.2211.10435},
	abstract = {Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("few-shot prompting"). Much of this success can be attributed to prompting methods such as "chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15\% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
	month = jan,
	year = {2023},
	note = {arXiv:2211.10435},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\JNTCB9L3\\Gao et al. - 2023 - PAL Program-aided Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\CL7ZP7NI\\2211.html:text/html},
}

@misc{jin_when_2022,
	title = {When to {Make} {Exceptions}: {Exploring} {Language} {Models} as {Accounts} of {Human} {Moral} {Judgment}},
	shorttitle = {When to {Make} {Exceptions}},
	url = {http://arxiv.org/abs/2210.01478},
	doi = {10.48550/arXiv.2210.01478},
	abstract = {AI systems are becoming increasingly intertwined with human life. In order to effectively collaborate with humans and ensure safety, AI systems need to be able to understand, interpret and predict human moral judgments and decisions. Human moral judgments are often guided by rules, but not always. A central challenge for AI safety is capturing the flexibility of the human moral mind -- the ability to determine when a rule should be broken, especially in novel or unusual situations. In this paper, we present a novel challenge set consisting of rule-breaking question answering (RBQA) of cases that involve potentially permissible rule-breaking -- inspired by recent moral psychology studies. Using a state-of-the-art large language model (LLM) as a basis, we propose a novel moral chain of thought (MORALCOT) prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments. MORALCOT outperforms seven existing LLMs by 6.2\% F1, suggesting that modeling human reasoning might be necessary to capture the flexibility of the human moral mind. We also conduct a detailed error analysis to suggest directions for future work to improve AI safety using RBQA. Our data is open-sourced at https://huggingface.co/datasets/feradauto/MoralExceptQA and code at https://github.com/feradauto/MoralCoT},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Jin, Zhijing and Levine, Sydney and Gonzalez, Fernando and Kamal, Ojasv and Sap, Maarten and Sachan, Mrinmaya and Mihalcea, Rada and Tenenbaum, Josh and Schölkopf, Bernhard},
	month = oct,
	year = {2022},
	note = {arXiv:2210.01478},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\YHH4TD4Q\\Jin et al. - 2022 - When to Make Exceptions Exploring Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\BGPU3HZF\\2210.html:text/html},
}

@misc{chen2024surveylargelanguagemodels,
      title={A Survey on Large Language Models for Critical Societal Domains: Finance, Healthcare, and Law}, 
      author={Zhiyu Zoey Chen and Jing Ma and Xinlu Zhang and Nan Hao and An Yan and Armineh Nourbakhsh and Xianjun Yang and Julian McAuley and Linda Petzold and William Yang Wang},
      year={2024},
      eprint={2405.01769},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.01769}, 
}
@misc{shen2023largelanguagemodelalignment,
      title={Large Language Model Alignment: A Survey}, 
      author={Tianhao Shen and Renren Jin and Yufei Huang and Chuang Liu and Weilong Dong and Zishan Guo and Xinwei Wu and Yan Liu and Deyi Xiong},
      year={2023},
      eprint={2309.15025},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.15025}, 
}

@misc{noauthor_next_nodate,
	title = {Next {Stop}: ‘{Trolley} {Problem}’},
	shorttitle = {Next {Stop}},
	url = {https://www.merriam-webster.com/wordplay/trolley-problem-moral-philosophy-ethics},
	abstract = {We have a hard decision to make.},
	language = {en},
	urldate = {2024-11-06},
	file = {Snapshot:C\:\\Users\\JC\\Zotero\\storage\\YQLC5EUM\\trolley-problem-moral-philosophy-ethics.html:text/html},
}


@article{bentham_introduction_nodate,
	title = {An {Introduction} to the {Principles} of {Morals} and {Legislation}},
	language = {en},
    year={1789},
	author = {Bentham, Jeremy},
	file = {Bentham - An Introduction to the Principles of Morals and Le.pdf:C\:\\Users\\JC\\Zotero\\storage\\XBZ7RHPW\\Bentham - An Introduction to the Principles of Morals and Le.pdf:application/pdf},
}
@misc{huang2023reasoninglargelanguagemodels,
      title={Towards Reasoning in Large Language Models: A Survey}, 
      author={Jie Huang and Kevin Chen-Chuan Chang},
      year={2023},
      eprint={2212.10403},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.10403}, 
}

@article{hasselberger_ethics_2019,
	title = {Ethics beyond {Computation}: {Why} {We} {Can}'t (and {Shouldn}'t) {Replace} {Human} {Moral} {Judgment} with {Algorithms}},
	volume = {86},
	issn = {1944-768X},
	shorttitle = {Ethics beyond {Computation}},
	url = {https://muse.jhu.edu/pub/1/article/748873},
	number = {4},
	urldate = {2024-11-05},
	journal = {Social Research: An International Quarterly},
	author = {Hasselberger, William},
	year = {2019},
	note = {Publisher: Johns Hopkins University Press},
	pages = {977--999},
}

@misc{noauthor_ethics_nodate,
	title = {Ethics beyond {Computation}: {Why} {We} {Can}'t (and {Shouldn}'t) {Replace} {Human} {Moral} {Judgment} with {Algorithms}},
	url = {https://www.researchgate.net/publication/344176411_Ethics_beyond_Computation_Why_We_Can't_and_Shouldn't_Replace_Human_Moral_Judgment_with_Algorithms},
	urldate = {2024-11-05},
	file = {Ethics beyond Computation\: Why We Can't (and Shouldn't) Replace Human Moral Judgment with Algorithms:C\:\\Users\\JC\\Zotero\\storage\\QNIHBA27\\344176411_Ethics_beyond_Computation_Why_We_Can't_and_Shouldn't_Replace_Human_Moral_Judgment_wit.html:text/html},
}

@inproceedings{talat_machine_2022,
	address = {Seattle, United States},
	title = {On the {Machine} {Learning} of {Ethical} {Judgments} from {Natural} {Language}},
	url = {https://aclanthology.org/2022.naacl-main.56},
	doi = {10.18653/v1/2022.naacl-main.56},
	abstract = {Ethics is one of the longest standing intellectual endeavors of humanity. In recent years, the fields of AI and NLP have attempted to address issues of harmful outcomes in machine learning systems that are made to interface with humans. One recent approach in this vein is the construction of NLP morality models that can take in arbitrary text and output a moral judgment about the situation described. In this work, we offer a critique of such NLP methods for automating ethical decision-making. Through an audit of recent work on computational approaches for predicting morality, we examine the broader issues that arise from such efforts. We conclude with a discussion of how machine ethics could usefully proceed in NLP, by focusing on current and near-future uses of technology, in a way that centers around transparency, democratic values, and allows for straightforward accountability.},
	urldate = {2024-11-05},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Talat, Zeerak and Blix, Hagen and Valvoda, Josef and Ganesh, Maya Indira and Cotterell, Ryan and Williams, Adina},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	pages = {769--779},
	file = {Full Text PDF:C\:\\Users\\JC\\Zotero\\storage\\6NWMVLCC\\Talat et al. - 2022 - On the Machine Learning of Ethical Judgments from .pdf:application/pdf},
}

@misc{noauthor_221210403_nodate,
	title = {[2212.10403] {Towards} {Reasoning} in {Large} {Language} {Models}: {A} {Survey}},
	url = {https://arxiv.org/abs/2212.10403},
	urldate = {2024-11-05},
	file = {[2212.10403] Towards Reasoning in Large Language Models\: A Survey:C\:\\Users\\JC\\Zotero\\storage\\TAJ9HRJ6\\2212.html:text/html},
}

@misc{zhou_rethinking_2024,
	title = {Rethinking {Machine} {Ethics} -- {Can} {LLMs} {Perform} {Moral} {Reasoning} through the {Lens} of {Moral} {Theories}?},
	url = {http://arxiv.org/abs/2308.15399},
	doi = {10.48550/arXiv.2308.15399},
	abstract = {Making moral judgments is an essential step toward developing ethical AI systems. Prevalent approaches are mostly implemented in a bottom-up manner, which uses a large set of annotated data to train models based on crowd-sourced opinions about morality. These approaches have been criticized for overgeneralizing the moral stances of a limited group of annotators and lacking explainability. This work proposes a flexible top-down framework to steer (Large) Language Models (LMs) to perform moral reasoning with well-established moral theories from interdisciplinary research. The theory-guided top-down framework can incorporate various moral theories. Our experiments demonstrate the effectiveness of the proposed framework on datasets derived from moral theories. Furthermore, we show the alignment between different moral theories and existing morality datasets. Our analysis exhibits the potential and flaws in existing resources (models and datasets) in developing explainable moral judgment-making systems.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Zhou, Jingyan and Hu, Minda and Li, Junan and Zhang, Xiaoying and Wu, Xixin and King, Irwin and Meng, Helen},
	month = jul,
	year = {2024},
	note = {arXiv:2308.15399},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\GECBDTJH\\Zhou et al. - 2024 - Rethinking Machine Ethics -- Can LLMs Perform Mora.pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\RSWVSCX3\\2308.html:text/html},
}

@misc{hendrycks_aligning_2023,
	title = {Aligning {AI} {With} {Shared} {Human} {Values}},
	url = {http://arxiv.org/abs/2008.02275},
	doi = {10.48550/arXiv.2008.02275},
	abstract = {We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry and Song, Dawn and Steinhardt, Jacob},
	month = feb,
	year = {2023},
	note = {arXiv:2008.02275},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\RNKU39T7\\Hendrycks et al. - 2023 - Aligning AI With Shared Human Values.pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\W8QML67X\\2008.html:text/html},
}

@misc{emelin_moral_2020,
	title = {Moral {Stories}: {Situated} {Reasoning} about {Norms}, {Intents}, {Actions}, and their {Consequences}},
	shorttitle = {Moral {Stories}},
	url = {http://arxiv.org/abs/2012.15738},
	doi = {10.48550/arXiv.2012.15738},
	abstract = {In social settings, much of human behavior is governed by unspoken rules of conduct. For artificial systems to be fully integrated into social environments, adherence to such norms is a central prerequisite. We investigate whether contemporary NLG models can function as behavioral priors for systems deployed in social settings by generating action hypotheses that achieve predefined goals under moral constraints. Moreover, we examine if models can anticipate likely consequences of (im)moral actions, or explain why certain actions are preferable by generating relevant norms. For this purpose, we introduce 'Moral Stories', a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. Finally, we propose decoding strategies that effectively combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines, e.g. though abductive reasoning.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Emelin, Denis and Bras, Ronan Le and Hwang, Jena D. and Forbes, Maxwell and Choi, Yejin},
	month = dec,
	year = {2020},
	note = {arXiv:2012.15738},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\8KN6CZYI\\Emelin et al. - 2020 - Moral Stories Situated Reasoning about Norms, Int.pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\KDRZIKE3\\2012.html:text/html},
}

@article{radford_language_nodate,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:C\:\\Users\\JC\\Zotero\\storage\\KQ4RAH46\\Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\7UJK8WCD\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\YGQRN4XQ\\2005.html:text/html},
}

@misc{klingefjord2024humanvaluesalignai,
      title={What are human values, and how do we align AI to them?}, 
      author={Oliver Klingefjord and Ryan Lowe and Joe Edelman},
      year={2024},
      eprint={2404.10636},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2404.10636}, 
}
@misc{openai_gpt-4_2024,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	month = mar,
	year = {2024},
	note = {arXiv:2303.08774},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\JC\\Zotero\\storage\\VI5HYRC5\\OpenAI et al. - 2024 - GPT-4 Technical Report.pdf:application/pdf;Snapshot:C\:\\Users\\JC\\Zotero\\storage\\WMY5YRJT\\2303.html:text/html},
}

@misc{noauthor_llama_nodate,
	title = {Llama 3.2: {Revolutionizing} edge {AI} and vision with open, customizable models},
	shorttitle = {Llama 3.2},
	url = {https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/},
	abstract = {Today, we’re releasing Llama 3.2, which includes small and medium-sized vision LLMs, and lightweight, text-only models that fit onto edge and mobile devices.},
	language = {en},
	urldate = {2024-11-05},
	journal = {Meta AI},
	file = {Snapshot:C\:\\Users\\JC\\Zotero\\storage\\EX8GHCPV\\llama-3-2-connect-2024-vision-edge-mobile-devices.html:text/html},
}
